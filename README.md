# Linguistic Corpus Analysis with NLTK

## ğŸ¯ Obiettivo

Realizzazione di due programmi scritti in Python (o tramite Jupyter/Colab Notebook) che utilizzano i moduli di **NLTK** per analizzare linguisticamente due **corpora di testo in lingua inglese**, confrontarli sulla base di alcuni indici statistici ed estrarre informazioni rilevanti.

---

## ğŸ“š Creazione dei Corpora

Ogni corpus deve rispettare le seguenti caratteristiche:

- Essere in **lingua inglese**
- Contenere **almeno 5000 parole**
- Rappresentare un **genere testuale specifico**  
  *(es. giornalismo, narrativa, blog, social media, articoli scientifici, ecc.)*
- Essere salvato in un **file di testo semplice** (`.txt`) con codifica **UTF-8**

---

## ğŸ› ï¸ Programmi da Sviluppare

Ogni programma deve:

- Leggere i file dei due corpora
- Effettuare **annotazione linguistica** (almeno fino al *Part-of-Speech Tagging*)

### ğŸ“¥ ModalitÃ  di Input

- Se sviluppato in **Python** (`.py`): i file devono essere passati da **riga di comando**
- Se sviluppato come **Notebook** (Jupyter/Colab): Ã¨ accettabile specificare il/i file nel codice, utilizzando un **percorso relativo**

---

## ğŸ“Š Programma 1 â€“ Analisi e Confronto

Il programma deve eseguire le seguenti operazioni:

- Sentence splitting  
- Tokenizzazione  
- POS tagging  
- Lemmatizzazione  

### âš–ï¸ Confronto tra i due corpora rispetto a:

1. **Numero totale di frasi e token**
2. **Lunghezza media delle frasi** (in token)
3. **Lunghezza media dei token**, escludendo la punteggiatura (in caratteri)
4. **Numero di hapax** (parole che compaiono una sola volta):
   - Nei primi 500 token  
   - Nei primi 1000 token  
   - Nei primi 3000 token  
   - Nellâ€™intero corpus
5. **Dimensione del vocabolario** e **ricchezza lessicale** (Type-Token Ratio, *TTR*) calcolata su porzioni incrementali di 200 token:  
   *200, 400, 600, ..., fino allâ€™intero testo*
6. **Numero di lemmi distinti** (*dimensione del vocabolario lemmatizzato*)
7. **Distribuzione delle frasi per polaritÃ **:
   - Utilizzando un **classificatore di polaritÃ ** fornito a lezione  
   - Oppure un **classificatore personalizzato**

---

## ğŸ§  Programma 2 â€“ Estrazione Statistiche e Pattern Linguistici

Il secondo programma prende in input **un singolo corpus** e, dopo aver effettuato le operazioni di annotazione linguistica (sentence splitting, tokenizzazione, POS tagging), deve **estrarre le seguenti informazioni**:

### 1. ğŸ“Œ Frequenze lessicali
- I **top-50 sostantivi**, **avverbi** e **aggettivi** piÃ¹ frequenti  
  > Visualizzare frequenza assoluta e relativa, ordinate per frequenza decrescente

### 2. ğŸ”¢ N-grammi piÃ¹ frequenti
- I **top-20 n-grammi di token** (con frequenza relativa) per:  
  - **n = 1, 2, 3, 4, 5**

### 3. âœï¸ N-grammi di PoS piÃ¹ frequenti
- I **top-20 n-grammi di Part-of-Speech (PoS)** per:  
  - **n = 1, 2, 3**  
  > Anche qui con frequenza relativa e ordinati per frequenza decrescente

### 4. ğŸ”— Bigranmi Aggettivoâ€“Sostantivo
I **top-10 bigrammi composti da aggettivo + sostantivo**, ordinati secondo:

a. Frequenza decrescente  
b. **Massima probabilitÃ  condizionata**, con valore associato  
c. **Massima probabilitÃ  congiunta**, con valore associato  
d. **Mutual Information (MI)** massima, con valore associato  
e. **Local Mutual Information (LMI)** massima, con valore associato  
f. Calcolo e stampa del **numero di elementi comuni** tra i top-10 per MI e LMI

### 5. ğŸ§© Analisi di Frasi Selezionate

Analizzare **le frasi comprese tra 10 e 20 token**, in cui almeno la **metÃ  dei token (parte intera)** ricorre **almeno 2 volte nel corpus**. Per tali frasi identificare:

- a. La frase con la **media piÃ¹ alta** della distribuzione di frequenza dei token  
- b. La frase con la **media piÃ¹ bassa**  
- c. La frase con la **probabilitÃ  piÃ¹ alta secondo un modello di Markov** di **ordine 2**

> ğŸ“Œ La media della distribuzione di frequenza dei token si ottiene sommando le frequenze dei token nella frase e dividendo per il numero di token.

### 6. ğŸ·ï¸ Named Entity Recognition (NER)
- Estrazione delle **EntitÃ  Nominate (Named Entities)** del corpus
- Per ogni **classe di entitÃ **, individuare i **15 elementi piÃ¹ frequenti**, ordinati per frequenza decrescente (con frequenza relativa)

---

## ğŸ“¦ Requisiti per la Consegna

PerchÃ© il progetto sia considerato **idoneo**, devono essere consegnati i seguenti materiali **in una cartella compressa**:

1. I **due corpora**, come file `.txt` in UTF-8  
2. I **due programmi o notebook**, **ben commentati**

### ğŸ–¥ï¸ Se il codice Ã¨ sviluppato in `.py` (Python script)

- Salvare i **risultati dellâ€™esecuzione** in un **file di testo formattato**
- Totale: **3 file di output**  
  - 1 per il **primo programma**  
  - 2 per il **secondo programma** (uno per ciascun corpus)

### ğŸ““ Se il codice Ã¨ sviluppato in Jupyter/Colab Notebook

- Consegnare i notebook **giÃ  eseguiti** (`.ipynb`)
- Totale: **3 notebook**  
  - 1 per il **primo programma**  
  - 2 copie del notebook del **secondo programma**: una per ogni corpus

âš ï¸ **Il codice deve essere eseguibile**, con attenzione ai *path* utilizzati (preferibilmente relativi), e deve replicare i risultati consegnati.

---
